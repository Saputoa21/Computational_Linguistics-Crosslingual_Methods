{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b42da2cc3c64299a03317968d69837b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bf69e6ee3db4d89a1a4f363516061a6",
              "IPY_MODEL_a73038051cae4c5aa2f225a19a30679d",
              "IPY_MODEL_79cb7b63eade4f5c98c3a53740ae343c"
            ],
            "layout": "IPY_MODEL_0ec946e43d3a4ca795689b282a43a5e5"
          }
        },
        "4bf69e6ee3db4d89a1a4f363516061a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1aa66fcc791437e82b6c7ad7fd5c13e",
            "placeholder": "​",
            "style": "IPY_MODEL_1cdb5746beae47d6ab9a147a2745f219",
            "value": "Map: 100%"
          }
        },
        "a73038051cae4c5aa2f225a19a30679d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30e270daf0a42a0a86edf2e2ed4f202",
            "max": 1043,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88e52be6e9e04cfabdfe62ee5740da58",
            "value": 1043
          }
        },
        "79cb7b63eade4f5c98c3a53740ae343c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f82328fff8c04d46af2744aa2a319d7c",
            "placeholder": "​",
            "style": "IPY_MODEL_72743fab2a7d4348bea8460856b1105a",
            "value": " 1043/1043 [00:00&lt;00:00, 1978.54 examples/s]"
          }
        },
        "0ec946e43d3a4ca795689b282a43a5e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1aa66fcc791437e82b6c7ad7fd5c13e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cdb5746beae47d6ab9a147a2745f219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a30e270daf0a42a0a86edf2e2ed4f202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e52be6e9e04cfabdfe62ee5740da58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f82328fff8c04d46af2744aa2a319d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72743fab2a7d4348bea8460856b1105a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saputoa21/Computational_Linguistics-Crosslingual_Methods/blob/main/Saputo_Bonus_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Exercises 2: Low-Rank Adaptation and Crosslingual Transfer**\n",
        "\n",
        "\n",
        "\n",
        "This notebook represents the second bonus exercises for the lecture Multilingual and Crosslingual Methods and Language Resources (2024W 340168-1). For each successfully completed bonus exercise, a maximum of three points can be achieved that will be added to the points of the final exam. The tasks to be completed in the following notebook are marked with 👋 ⚒.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_RsHNVC57Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will use Low-Rank Adaptation to Fine-Tune XLM-R on the task of linguistic acceptability in English and then test its zero-shot capability in other languages.\n",
        "\n",
        "# **Make sure to set your runtime to GPU before you start training.**\n",
        "\n",
        "(Tab: Runtime/Change Runtime Type -> Select GPU)"
      ],
      "metadata": {
        "id": "v5cE7oECGoNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Fine-Tuning on English**\n",
        "\n",
        "The first part has already been prepared for you. We will load and preprocess the Corpus for Linguistic Acceptability (COLA) dataset from GLUE and then use Low-Rank Adaptation to fine-tune XLM-R."
      ],
      "metadata": {
        "id": "lmoyUnpZKmma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "\n",
        "As always, we first need to install the necessary libraries. One that is new in this notebook is the Parameter-Efficient Fine-Tuning (PEFT) library."
      ],
      "metadata": {
        "id": "_RWUW8a4djOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U evaluate\n",
        "!pip install -U datasets\n",
        "!pip install -U transformers\n",
        "!pip install -U peft"
      ],
      "metadata": {
        "id": "IRnus8Ljb1QX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05e6617-e964-40b2-d63b-f3b48addb4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.48.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.2.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "In this notebook we will first be using the COLA dataset from the GLUE library and then a multilingual extension.\n",
        " We will first train on English and transfer to another language and evaluate zero-shot transfer on one more language (see [here](https://huggingface.co/datasets/Geralt-Targaryen/MELA) for a selection)."
      ],
      "metadata": {
        "id": "UT-WD8SmAvkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_en = load_dataset(\"glue\", \"cola\")\n",
        "dataset_en.num_rows"
      ],
      "metadata": {
        "id": "snp8ndm5gZEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b27077-47cf-4ebc-9fd0-976f717fa4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 8551, 'validation': 1043, 'test': 1063}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us take a look at the components of the dataset."
      ],
      "metadata": {
        "id": "9d5HMENBesKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_en['train'].features"
      ],
      "metadata": {
        "id": "R1zcS7FtevB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b85e02-1373-425e-9d2a-aeb7e9775d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['unacceptable', 'acceptable'], id=None),\n",
              " 'idx': Value(dtype='int32', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. To enable the conversion between various third-party libraries, Hugging Face Datasets provides a Dataset.set_format() function. This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format which is Apache Arrow. The formatting is done in-place, so let’s convert our dataset to Pandas and look at a random sample:"
      ],
      "metadata": {
        "id": "kuTCNH3Fe61B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "dataset_en.set_format(\"pandas\")\n",
        "df = dataset_en[\"train\"][:]\n",
        "# Create a random sample\n",
        "sample = df.sample(n=5, random_state=42)\n",
        "display(HTML(sample.to_html()))"
      ],
      "metadata": {
        "id": "FN4bCWnce7RM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1633cd2d-45da-416f-ca8d-465fa8814093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "      <th>idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>Angela characterized Shelly as a lifesaver.</td>\n",
              "      <td>1</td>\n",
              "      <td>2389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5048</th>\n",
              "      <td>They're not finding it a stress being in the same office.</td>\n",
              "      <td>1</td>\n",
              "      <td>5048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>Paul exhaled on Mary.</td>\n",
              "      <td>0</td>\n",
              "      <td>3133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>I ordered if John drink his beer.</td>\n",
              "      <td>0</td>\n",
              "      <td>5955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>Press the stamp against the pad completely.</td>\n",
              "      <td>1</td>\n",
              "      <td>625</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pandas dataframe can now be used as we would always use Pandas, for instance to count the number of labels for `cause` in the column question."
      ],
      "metadata": {
        "id": "kB2qLMNvfhcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "N2pCWD7JfgQY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "e8b84a2d-0ada-43b3-a59f-b680dc14b35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    6023\n",
              "0    2528\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2528</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the two labels are spread quite evenly across the two types of questions.\n",
        "\n",
        "This was just a brief detour to show how datasets can be nicely manipulated and displayed using other libraries. We will now get back to our usual datasets library from Hugging Face. To this end, we will reset the format."
      ],
      "metadata": {
        "id": "6dVdd-W7f8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_en.reset_format()"
      ],
      "metadata": {
        "id": "OwPKWfq1hLoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Dataset\n",
        "\n",
        "In this example, we model COPA as a multiple-choice task with two choices. Thus, we encode the premise and question as well as both choices as one input to our `xlm-roberta-base` model. Using `dataset.map()`, we can pass the full dataset through the tokenizer in batches."
      ],
      "metadata": {
        "id": "wCujQTNRLHFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
        "batch_size = 32\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=True, truncation=True)\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "  token_dataset = dataset.map(tokenize_function, batched=True, batch_size=batch_size)\n",
        "  tokenized_dataset = token_dataset.rename_column(\"label\", \"labels\")\n",
        "  return tokenized_dataset\n",
        "\n",
        "data_set_en_with_test = DatasetDict(\n",
        "    train=dataset_en['train'].shuffle(seed=24).select(range(7488)),\n",
        "    validation=dataset_en['validation'],\n",
        "    test=dataset_en['train'].shuffle(seed=24).select(range(7488, 8551)),\n",
        ")\n",
        "\n",
        "tokenized_dataset_en = preprocess_dataset(data_set_en_with_test)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ei3DaO3mgnX5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6b42da2cc3c64299a03317968d69837b",
            "4bf69e6ee3db4d89a1a4f363516061a6",
            "a73038051cae4c5aa2f225a19a30679d",
            "79cb7b63eade4f5c98c3a53740ae343c",
            "0ec946e43d3a4ca795689b282a43a5e5",
            "d1aa66fcc791437e82b6c7ad7fd5c13e",
            "1cdb5746beae47d6ab9a147a2745f219",
            "a30e270daf0a42a0a86edf2e2ed4f202",
            "88e52be6e9e04cfabdfe62ee5740da58",
            "f82328fff8c04d46af2744aa2a319d7c",
            "72743fab2a7d4348bea8460856b1105a"
          ]
        },
        "outputId": "09473097-6fc2-43f9-cee7-efd41e9d0d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1043 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b42da2cc3c64299a03317968d69837b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_en[\"train\"][1]"
      ],
      "metadata": {
        "id": "q7q0udiH0TRa",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3312d9-e156-4575-f951-c569e10b66fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'John knows that she left and whether she will come back.',\n",
              " 'labels': 1,\n",
              " 'idx': 7246,\n",
              " 'input_ids': [0,\n",
              "  4939,\n",
              "  93002,\n",
              "  450,\n",
              "  2412,\n",
              "  25737,\n",
              "  136,\n",
              "  36766,\n",
              "  2412,\n",
              "  1221,\n",
              "  1380,\n",
              "  4420,\n",
              "  5,\n",
              "  2,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_en[\"train\"][\"sentence\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6ApxAI6143g9",
        "outputId": "50354116-561f-4b2e-fa00-8bbfa2148854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'John knows that she left and whether she will come back.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_en[\"train\"][\"labels\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBoh9huH46Z6",
        "outputId": "2f656f5d-de52-4275-cf6e-73d7c3141378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Low-Rank Adaptation (LoRA)**\n",
        "\n",
        "In order to perform low-rank adaptation (LoRA) on a pretrained language model for parameter-efficient fine-tuning (PEFT), we need to set a few parameters in the LoRA Configuration. Hugging Face offers some [documentation on LoRA](https://huggingface.co/docs/peft/main/en/developer_guides/lora).\n",
        "\n",
        "The `task-type` specifies which task the model should be fine-tuned on and needs to correspond to the way the model is loaded. If we load a model for Sequence Classification, also the task needs to be `SEQ_CLS`, an abbreviation for Sequence Classification. Then the dataset needs to be one with an input sequence and a number of target classes.\n",
        "\n",
        "The `target-module` depends on the type of model, which for XLM-R is `[\"query\", \"value\"]`. Since we wish to change model parameters, the inference mode is set to false. The variable `r`indicates the rank to which the dimensionality is being reduced. The variable `alpha` is a scaling parameter, because `r`scales at 1.0. With small datasets or if unsure, the rank and alpha can be the same. Finally, dropout is a random omission of trainable parameters (setting to zero) during training, mostly to avoid overfitting.\n",
        "\n",
        "Feel free to play with and adapt these parameters if you are interested in seeing the effect.\n"
      ],
      "metadata": {
        "id": "fZ8nhE-4LXNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftType, get_peft_model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "peft_type = PeftType.LORA\n",
        "peft_config = LoraConfig(task_type=\"SEQ_CLS\", target_modules=[\"query\", \"value\"], inference_mode=False, r=8, lora_alpha=8, lora_dropout=0.1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-large\")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model"
      ],
      "metadata": {
        "id": "mmA3RxaSg3-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6b550d-b3e6-402e-fd3a-e150c6b0bc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,838,082 || all params: 561,730,564 || trainable%: 0.3272\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): XLMRobertaForSequenceClassification(\n",
              "      (roberta): XLMRobertaModel(\n",
              "        (embeddings): XLMRobertaEmbeddings(\n",
              "          (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
              "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "          (token_type_embeddings): Embedding(1, 1024)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder): XLMRobertaEncoder(\n",
              "          (layer): ModuleList(\n",
              "            (0-23): 24 x XLMRobertaLayer(\n",
              "              (attention): XLMRobertaAttention(\n",
              "                (self): XLMRobertaSdpaSelfAttention(\n",
              "                  (query): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                  (value): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): XLMRobertaSelfOutput(\n",
              "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate): XLMRobertaIntermediate(\n",
              "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): XLMRobertaOutput(\n",
              "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (classifier): ModulesToSaveWrapper(\n",
              "        (original_module): XLMRobertaClassificationHead(\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
              "        )\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): XLMRobertaClassificationHead(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "num_train_epochs = 5\n",
        "logging_steps = len(tokenized_dataset_en[\"train\"]) // (batch_size * num_train_epochs)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='none',\n",
        "    load_best_model_at_end=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset_en[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_en[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "l-SNWkDnihuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have configured the model with PEFT, we can train the PEFT model as usual."
      ],
      "metadata": {
        "id": "RG5HncHoRIAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "sGIdhKGgjkIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "bf9811ca-41ce-412b-e288-05da3c437c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1170' max='1170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1170/1170 09:44, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.587700</td>\n",
              "      <td>0.598689</td>\n",
              "      <td>0.693193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.519800</td>\n",
              "      <td>0.495831</td>\n",
              "      <td>0.761266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.480400</td>\n",
              "      <td>0.479715</td>\n",
              "      <td>0.782359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.441600</td>\n",
              "      <td>0.507901</td>\n",
              "      <td>0.778523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.395200</td>\n",
              "      <td>0.495295</td>\n",
              "      <td>0.797699</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1170, training_loss=0.509758540096446, metrics={'train_runtime': 586.7646, 'train_samples_per_second': 63.808, 'train_steps_per_second': 1.994, 'total_flos': 2688914965511424.0, 'train_loss': 0.509758540096446, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Evaluate on the English test set to see how well the fine-tuning has worked."
      ],
      "metadata": {
        "id": "LXIkfddAH4fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for the evaluation here\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"/content/training_output/checkpoint-1170\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)\n",
        "\n",
        "model_inputs = tokenizer(\"I ordered if John dink his beer.\", return_tensors=\"pt\")\n",
        "outputs = inference_model(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "id": "B84dxesrJH-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47be5d79-9c99-4cb4-920d-cebe5dc371e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0])\n",
            "unacceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "eval_dataloader = DataLoader(tokenized_dataset_en['test'], batch_size=8)\n",
        "\n",
        "inference_model.eval()\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "    input = tokenizer(batch['sentence'], padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model(**input)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
        "\n",
        "accuracy.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLh0IY9G2-dS",
        "outputId": "efcd83df-4da7-4d88-8469-16b773d53b28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8052681091251176}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Crosslingual Transfer**\n",
        "\n",
        "In this section, we will be using the Multilingual Evaluation of Linguistic Acceptability ([MELA](https://github.com/sjtu-compling/mela?tab=readme-ov-file)), which is also [available on Hugging Face](https://huggingface.co/datasets/Geralt-Targaryen/MELA) to test the transfer and zero-shot capabilities of XLM-R with LoRA Fine-Tuning.\n",
        "\n",
        "We will first fine-tune on German and then test the on German but also in a zero-shot approach on another language of your choice.\n",
        "\n",
        "Please be aware of the fact that MELA \"only\" offers a dev and a test set - no train, validation, test split. Thus, the preprocessing needs to be slightly adapted."
      ],
      "metadata": {
        "id": "LfVSDWxbGaAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "de = load_dataset(\"Geralt-Targaryen/MELA\", \"de\")\n",
        "dataset_de = preprocess_dataset(de)\n",
        "batch_size = 16\n",
        "print(dataset_de[\"test\"][20])"
      ],
      "metadata": {
        "id": "-Ad2i7j49qdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57dcb8c-e98e-4d26-ec99-75490ab415ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'idx': 'c1-1.1_n9-a', 'labels': 1, 'sentence': 'Wenn du glaubst, dass er sich geirrt habe, kannst du dann alles verstehen', 'input_ids': [0, 7896, 115, 24682, 5829, 271, 4, 1421, 72, 833, 6, 128696, 3198, 3260, 4, 32540, 115, 3700, 4174, 85516, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_de[\"dev\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUrK6m9h5OXb",
        "outputId": "6e5e60f6-0329-4b26-d275-5a1d5933ddd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'idx': 'c5-5.1_n1-f-1', 'labels': 1, 'sentence': 'Er hat nicht ausgeschlossen, dass es so gewesen sein könnte ', 'input_ids': [0, 1004, 1256, 749, 206941, 4, 1421, 198, 221, 72888, 2988, 25482, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Use the German dev partition to further-finetune the previously configured model and then evaluate on the test partition of the German dataset."
      ],
      "metadata": {
        "id": "rC4WwjbeKGFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning on German dev set here\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "num_train_epochs = 5\n",
        "logging_steps = len(dataset_de[\"dev\"]) // (batch_size * num_train_epochs)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    output_dir=\"./training_output2\",\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='none',\n",
        "    load_best_model_at_end=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=inference_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_de[\"dev\"],\n",
        "    eval_dataset=dataset_de[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "x0H1RhyqKFG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "aHXUYBpt4wxD",
        "outputId": "02158a3d-7231-4af8-fb20-a05466b33c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:30, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.001000</td>\n",
              "      <td>0.526841</td>\n",
              "      <td>0.738624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.430700</td>\n",
              "      <td>0.549932</td>\n",
              "      <td>0.715344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.332000</td>\n",
              "      <td>0.485426</td>\n",
              "      <td>0.759788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.467700</td>\n",
              "      <td>0.468200</td>\n",
              "      <td>0.778836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.365600</td>\n",
              "      <td>0.469248</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=35, training_loss=0.7001416385173798, metrics={'train_runtime': 31.0494, 'train_samples_per_second': 16.103, 'train_steps_per_second': 1.127, 'total_flos': 22890086700000.0, 'train_loss': 0.7001416385173798, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on German Dataset"
      ],
      "metadata": {
        "id": "SuMAAXEL8k8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id_de = \"/content/training_output2/checkpoint-28\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id_de)\n",
        "inference_model_de = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "inference_model_de = PeftModel.from_pretrained(inference_model_de, peft_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkN6BaxFEGmt",
        "outputId": "27aebfa2-b03a-48c7-aa48-7ae6ee210f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Er sagt, dass man diesen Satz grammatisch machen kann.\", return_tensors=\"pt\") #labeled as 1\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ciHZfkt6hPV",
        "outputId": "cff50d59-cc9c-44a9-a371-b3a97161c7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Jeder, der hat gelacht.\", return_tensors=\"pt\") #labeled as 0\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AYUsD8279En",
        "outputId": "4eca9a0c-9e0a-4997-e654-7a71a5975fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "eval_dataloader = DataLoader(dataset_de['test'], batch_size=8)\n",
        "\n",
        "inference_model_de.eval()\n",
        "\n",
        "for batch in eval_dataloader:\n",
        "    input = tokenizer(batch['sentence'], padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model_de(**input)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
        "\n",
        "accuracy.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR2JJYXt7SE5",
        "outputId": "8bf9b54a-2815-4a0c-f3da-846bc7f28dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7587301587301587}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "👋 ⚒ Select another language of your choice from the [MELA dataset](https://huggingface.co/datasets/Geralt-Targaryen/MELA) to only evaluate the fine-tuned model (zero-shot capability).\n",
        "\n",
        "**Alternative**: Feel free to create your own mini-dataset of a few (non)-acceptable sentences in a language of your choice to test the model's zero-shot capacity."
      ],
      "metadata": {
        "id": "iEbyOVU6KqGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Russian examples to evaluate model's performace on a language from a differenet language family (an East Slavic language)"
      ],
      "metadata": {
        "id": "HHChPZy3IDQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Проснешься не торопясь, посердишься на что-нибудь, поворчишь.\", return_tensors=\"pt\") #labeled as 1\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt1KSt6kICqQ",
        "outputId": "545c0a64-6c70-4474-97c4-4b17fea58dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Те, кто мечтает стать инженером, исследователем, лётчиком, космонавтом, должен развивать свою зрительную память.\", return_tensors=\"pt\") #labeled as 0\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8ISnapYISfe",
        "outputId": "01af02aa-8e8d-4ae3-e060-ffa071b3e2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Italian examples to evaluate model's performace on a language from a differenet language family (a Romance language)"
      ],
      "metadata": {
        "id": "vhVXY3mL8ZgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your evaluation here\n",
        "model_inputs = tokenizer(\"Tommaso legge il giornale.\", return_tensors=\"pt\") #labeled as 1\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "id": "UdYUU4MoK0jE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa70067d-a02e-47b4-87c7-9327393cec6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Uno studente parlato poco fa.\", return_tensors=\"pt\") #labeled as 0\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rS0b0UG7vNT",
        "outputId": "cb7f07b7-5b1f-4c06-88f2-a0335db75279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0])\n",
            "unacceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Islandic examples to evaluate model's performace on a language from a the same language family (a North Germanic language)"
      ],
      "metadata": {
        "id": "4cOu3lOZGxF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Það er ekki gott að vanta einan í tíma.\", return_tensors=\"pt\") #labeled as 1\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8hgc9GoGwo8",
        "outputId": "36574acd-8bf7-4d3c-fd17-ec18159fb8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(\"Þetta eru þessar bækur fjórar mínar.\", return_tensors=\"pt\") #labeled as 0\n",
        "outputs = inference_model_de(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBlX5ZMVG45N",
        "outputId": "b2c4b0f4-22b0-4827-e6fe-0d8b2cee17b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "acceptable\n"
          ]
        }
      ]
    }
  ]
}